{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"oAPcL0pRs29J"},"outputs":[],"source":["# NOTE FOR LOCAL EXECUTION: use pytorch kernel and execute the code below to specify cuda device\n","import os\n","os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RM-HTfuLX4RQ"},"outputs":[],"source":["# install packages\n","!pip install torch transformers memory_profiler datasets accelerate nltk tweet-preprocessor\n","import time\n","import datetime\n","tic = time.time()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"iOa0uy-SbMtR"},"outputs":[],"source":["# import modules\n","import torch\n","import random\n","from transformers.file_utils import is_tf_available, is_torch_available, is_torch_tpu_available\n","from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments, AdamW\n","from datasets import load_metric\n","%load_ext memory_profiler\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import accuracy_score\n","import pandas as pd\n","import numpy as np"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"nehTMnr6s29M"},"outputs":[],"source":["# Function to create partition from split\n","def create_partition_from_split(train, test, x_colname, y_colname):\n","    X_train = train[x_colname]\n","    X_test = test[x_colname]\n","    y_train = train[y_colname]\n","    y_test = test[y_colname]\n","    return X_train, X_test, y_train, y_test"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"IX7A4Y9TYFwa"},"outputs":[],"source":["# Function to define datasets\n","def create_datasets(tokenizer, X_train, X_test, y_train, y_test):\n","    # Convert input dataframes into lists\n","    X_train = X_train.tolist()\n","    X_test = X_test.tolist()\n","    y_train = y_train.tolist()\n","    y_test = y_test.tolist()\n","\n","    # Tokenize the text\n","    train_tokens = tokenizer(X_train, truncation=True, padding=True, max_length=512)\n","    valid_tokens = tokenizer(X_test, truncation=True, padding=True, max_length=512)\n","\n","    class MakeTorchData(torch.utils.data.Dataset):\n","        def __init__(self, tokens, labels):\n","            self.tokens = tokens\n","            self.labels = labels\n","\n","        def __getitem__(self, idx):\n","            item = {k: torch.tensor(v[idx]) for k, v in self.tokens.items()}\n","            item[\"labels\"] = torch.tensor([self.labels[idx]])\n","            return item\n","\n","        def __len__(self):\n","            return len(self.labels)\n","\n","    # Convert our tokenized data into a torch Dataset\n","    train_dataset = MakeTorchData(train_tokens, y_train)\n","    valid_dataset = MakeTorchData(valid_tokens, y_test)\n","\n","    return train_dataset, valid_dataset"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Ls6OMwxBYtgr"},"outputs":[],"source":["from sklearn.metrics import accuracy_score\n","auc_metric = load_metric(\"roc_auc\", trust_remote_code=True)\n","\n","def compute_metrics(eval_pred):\n","    # Calculate accuracy\n","    predictions, labels = eval_pred\n","    predictions = np.argmax(predictions, axis=1)\n","    accuracy = accuracy_score(y_true=labels, y_pred=predictions)\n","\n","    # Calculate AUC\n","    roc_auc = auc_metric.compute(prediction_scores=predictions, references=labels)[\"roc_auc\"]\n","\n","    return {\"accuracy\": accuracy, \"roc_auc\": roc_auc}"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"iHjUrRyUuReQ"},"outputs":[],"source":["from transformers import TrainerCallback\n","\n","class CustomCallback(TrainerCallback):\n","\n","    def __init__(self, trainer) -> None:\n","        super().__init__()\n","        self._trainer = trainer\n","\n","    def on_epoch_end(self, args, state, control, **kwargs):\n","        train_results = self._trainer.evaluate(eval_dataset=self._trainer.train_dataset, metric_key_prefix=\"train\")\n","        print(train_results)\n","        eval_results = self._trainer.evaluate(eval_dataset=self._trainer.eval_dataset, metric_key_prefix=\"eval\")\n","        print(eval_results)\n","        print()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"AFiVPzkwYKtc"},"outputs":[],"source":["# Create trainer\n","def create_trainer(suffix, model, train_dataset, valid_dataset, num_epochs=5,\n","                   load_best_model_at_end=True, optimizer=None):\n","    training_args = TrainingArguments(\n","      output_dir='./results_' + suffix,  # output directory\n","      num_train_epochs=num_epochs,       # total number of training epochs\n","      per_device_train_batch_size=16,     # batch size per device during training\n","      per_device_eval_batch_size=16,     # batch size for evaluation\n","      warmup_steps=500,                  # number of warmup steps for learning rate scheduler\n","      weight_decay=0.01,                 # strength of weight decay\n","      logging_dir='./logs_' + suffix,    # directory for storing logs\n","      load_best_model_at_end = load_best_model_at_end, # load the best model when finished training\n","      metric_for_best_model = \"eval_roc_auc\", # select the base metrics\n","      greater_is_better = True,\n","      logging_steps=100,                 # log & save weights each logging_steps\n","      #save_steps=100,\n","      #save_strategy=\"steps\",\n","      save_strategy=\"epoch\",\n","      save_total_limit=1,\n","      #evaluation_strategy=\"steps\",      # evaluate each `logging_steps`\n","      evaluation_strategy=\"epoch\",       # evaluate each epoch\n","    )\n","\n","    trainer = Trainer(\n","        model=model,                     # the instantiated Transformers model to be trained\n","        args=training_args,              # training arguments, defined above\n","        train_dataset=train_dataset,     # training dataset\n","        eval_dataset=valid_dataset,      # evaluation dataset\n","        compute_metrics=compute_metrics, # the callback that computes metrics of interest\n","        optimizers=(optimizer, None)\n","    )\n","    # NOTE: metric for best model not correctly applied when adding callback\n","    #trainer.add_callback(CustomCallback(trainer))\n","    return trainer"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_WodFCCDs29P"},"outputs":[],"source":["import torch.nn as nn\n","from sklearn.model_selection import KFold\n","\n","def write_float_pair_to_file(file_path, float1, float2):\n","    with open(file_path, 'w') as file:\n","        file.write(f\"{float1} {float2}\\n\")\n","\n","def get_mlp_classifier(hidden_size, num_labels, hsize_factor=1):\n","    new_classifier = nn.Sequential(\n","        nn.Linear(hidden_size, int(hidden_size * hsize_factor)),\n","        nn.ReLU(),\n","        nn.Linear(int(hidden_size * hsize_factor), num_labels)\n","    )\n","    return new_classifier\n","\n","def instantiate_model(model_name, num_labels, hidden_dropout_prob, attention_probs_dropout_prob,\n","                      mlp_classif=False, mlp_hsize_factor=1, freeze_bert=False):\n","    # Define the model with the current dropout probabilities\n","    if \"distilbert-\" in model_name:\n","        model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2,\n","                                                                   dropout=hidden_dropout_prob,\n","                                                                   attention_dropout=attention_probs_dropout_prob)\n","    else:\n","        model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2,\n","                                                                   hidden_dropout_prob=hidden_dropout_prob,\n","                                                                   attention_probs_dropout_prob=attention_probs_dropout_prob)\n","\n","    if mlp_classif:\n","        model.classifier = get_mlp_classifier(model.config.hidden_size,\n","                                              model.config.num_labels,\n","                                              mlp_hsize_factor)\n","\n","    # Freeze BERT parameters if requested\n","    if freeze_bert:\n","        for param in model.bert.parameters():\n","            param.requires_grad = False\n","\n","    return model\n","\n","def single_train_exper(suffix, hidden_dropout_prob, attention_probs_dropout_prob, lr, num_epochs,\n","                       model_name, train_dataset, valid_dataset, test_dataset=None, load_best_model_at_end=True,\n","                       mlp_classif=False, mlp_hsize_factor=1, freeze_bert=False):\n","    model = instantiate_model(model_name,\n","                              num_labels=2,\n","                              hidden_dropout_prob=hidden_dropout_prob,\n","                              attention_probs_dropout_prob=attention_probs_dropout_prob,\n","                              mlp_classif=mlp_classif,\n","                              mlp_hsize_factor=mlp_hsize_factor,\n","                              freeze_bert=freeze_bert)\n","\n","    optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=1e-2)\n","\n","\n","    # Rest of your code for this specific model configuration\n","    # Train the model\n","    trainer = create_trainer(suffix, model, train_dataset, valid_dataset, num_epochs=num_epochs,\n","                             load_best_model_at_end=load_best_model_at_end, optimizer=optimizer)\n","    training_results = trainer.train()  # Capture the training results\n","\n","    # Evaluate the model using the validation data\n","    valid_results = trainer.evaluate()\n","\n","    # Evaluate the model using the test data if given\n","    if test_dataset is not None:\n","        test_results = trainer.evaluate(eval_dataset=test_dataset)\n","    else:\n","        test_results = None\n","\n","    return trainer, training_results, valid_results, test_results\n","\n","\n","def training_experiment(suffix, hidden_dropout_probs, attn_dropout_probs, learn_rates, num_epochs,\n","                        model_name, train_dataset, valid_dataset, test_dataset=None, mlp_classif=False,\n","                        mlp_hsize_factor = 1, freeze_bert=False):\n","    # Loop to iterate through all combinations\n","    best_eval_measure = None\n","    best_test_measure = None\n","    for hidden_dropout_prob in hidden_dropout_probs:\n","        for attention_probs_dropout_prob in attn_dropout_probs:\n","            for lr in lrs:\n","                # Print start message\n","                print(f\"* Model with hidden_dropout_prob={hidden_dropout_prob} , attention_probs_dropout_prob={attention_probs_dropout_prob} and lr={lr}:\")\n","\n","                trainer, training_results, valid_results, test_results = single_train_exper(suffix,\n","                                                                                   hidden_dropout_prob,\n","                                                                                   attention_probs_dropout_prob,\n","                                                                                   lr,\n","                                                                                   num_epochs,\n","                                                                                   model_name,\n","                                                                                   train_dataset,\n","                                                                                   valid_dataset,\n","                                                                                   test_dataset,\n","                                                                                   load_best_model_at_end=True,\n","                                                                                   mlp_classif=mlp_classif,\n","                                                                                   mlp_hsize_factor=mlp_hsize_factor,\n","                                                                                   freeze_bert=freeze_bert)\n","\n","                # Show results\n","                print(f\"Training Results: {training_results}\")\n","                print(f\"Validation Results: {valid_results}\")\n","                if test_dataset is not None:\n","                    print(f\"Test Results: {test_results}\")\n","                print(\"\")\n","\n","                # Save model if it is the best so far\n","                eval_measure = valid_results['eval_roc_auc']\n","                if best_eval_measure is None or best_eval_measure < eval_measure:\n","                    best_eval_measure = eval_measure\n","                    if test_dataset is not None:\n","                        best_test_measure = test_results['eval_roc_auc']\n","                    trainer.save_model()\n","\n","    # Write validation and test AUCs to file\n","    output_dir='./results_' + suffix\n","    write_float_pair_to_file(output_dir+\"/aucs.txt\", best_eval_measure, best_test_measure)\n","\n","    return best_eval_measure, best_test_measure\n","\n","from sklearn.model_selection import KFold\n","\n","def kfold_exper(df, x_colname, y_colname, n_splits, suffix, hidden_dropout_prob,\n","                attention_probs_dropout_prob, lr, num_epochs, model_name,\n","                mlp_classif=False, mlp_hsize_factor = 1, freeze_bert=False):\n","    sum_eval_measure = 0\n","\n","    # Initialize KFold object\n","    kf = KFold(n_splits=n_splits, random_state=42, shuffle=True)\n","\n","    # Iterate over folds\n","    for i, (train_indices, test_indices) in enumerate(kf.split(df)):\n","        print(f\"Fold {i}:\")\n","\n","        # Obtain dataframes\n","        df_train = df.iloc[train_indices]\n","        df_test = df.iloc[test_indices]\n","\n","        # Obtain datasets\n","        X_train, X_test, y_train, y_test = create_partition_from_split(df_train, df_test, x_colname, y_colname)\n","        train_dataset, valid_dataset = create_datasets(X_train, X_test, y_train, y_test)\n","\n","        # Execute train experiment\n","        _, training_results, valid_results, _ = single_train_exper(suffix,\n","                                                                   hidden_dropout_prob,\n","                                                                   attention_probs_dropout_prob,\n","                                                                   lr,\n","                                                                   num_epochs,\n","                                                                   model_name,\n","                                                                   train_dataset,\n","                                                                   valid_dataset,\n","                                                                   load_best_model_at_end=False,\n","                                                                   mlp_classif=mlp_classif,\n","                                                                   mlp_hsize_factor=mlp_hsize_factor,\n","                                                                   freeze_bert=freeze_bert)\n","\n","        # Show results\n","        print(f\"Training Results: {training_results}\")\n","        print(f\"Validation Results: {valid_results}\")\n","        print(\"\")\n","\n","        # Retrieve evaluation measure\n","        eval_measure = valid_results['eval_roc_auc']\n","        sum_eval_measure += eval_measure\n","        print(\"- Evaluation measure:\", eval_measure)\n","        print(\"\")\n","\n","    result = sum_eval_measure / n_splits\n","    print(\"Average of Evaluation Measures:\", result)\n","\n","    return result"]},{"cell_type":"markdown","metadata":{"id":"HhUJDyUUs29R"},"source":["## Data Preparation"]},{"cell_type":"markdown","metadata":{"id":"UUwy2G2Qs29S"},"source":["### Load Original Data"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"IpWxeGPwN0ri"},"outputs":[],"source":["# Set the directory and create the data\n","\n","#from google.colab import drive\n","#drive.mount('/content/drive')\n","\n","# Load the data into a pandas DataFrame | remember to have a similar structure in your Drive so that the data can be read properly.\n","#df = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/NLP & Personality/essays.csv', encoding = \"latin-1\")\n","df = pd.read_csv('/home/dortiz/Dropbox/work/ub/research/projects/psico/data/essays_utf8.csv', encoding = \"utf-8\")\n","df = df.replace({'y': 1, 'n': 0})\n","\n","# Print a sample of the data to verify it's loaded correctly\n","print(df.head())"]},{"cell_type":"markdown","metadata":{"id":"05JYCN1rs29S"},"source":["### Preprocess Data"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ldIuEyFSs29T"},"outputs":[],"source":["import preprocessor as p\n","\n","df['TEXT'] = df['TEXT'].apply(p.clean)"]},{"cell_type":"markdown","metadata":{"id":"U9FDigxCs29T"},"source":["### Data Fragmentation"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"c4zraZmss29T"},"outputs":[],"source":["def fragment_essays_df(df, max_words, min_fragm_len):\n","\n","    def add_dict_entry(dict_of_lists, authid, text, cext, cneu, cagr, ccon, copn):\n","        dict_of_lists[\"#AUTHID\"].append(authid)\n","        dict_of_lists[\"TEXT\"].append(text)\n","        dict_of_lists[\"cEXT\"].append(cext)\n","        dict_of_lists[\"cNEU\"].append(cneu)\n","        dict_of_lists[\"cAGR\"].append(cagr)\n","        dict_of_lists[\"cCON\"].append(ccon)\n","        dict_of_lists[\"cOPN\"].append(copn)\n","\n","    def fragment_string(text, max_words):\n","        words = text.split()\n","        fragments = []\n","        current_fragment = []\n","\n","        for word in words:\n","            current_fragment.append(word)\n","            if len(current_fragment) == max_words:\n","                fragments.append(' '.join(current_fragment))\n","                current_fragment = []\n","\n","        if current_fragment:\n","            fragments.append(' '.join(current_fragment))\n","\n","        return fragments\n","\n","    # Convert DataFrame to a dictionary of lists\n","    dict_of_lists = df.to_dict(orient='list')\n","\n","    # Initialize fragmented dictionary\n","    fragm_dict_of_lists = {colname:[] for colname in dict_of_lists}\n","\n","    # Iterate over rows\n","    num_rows = len(dict_of_lists[\"TEXT\"])\n","\n","    # Initialize dictionary to memoize best synonyms\n","    best_synonym_map = {}\n","\n","    for i in range(num_rows):\n","        # Retrieve column values\n","        authid = dict_of_lists[\"#AUTHID\"][i]\n","        text = dict_of_lists[\"TEXT\"][i]\n","        cext = dict_of_lists[\"cEXT\"][i]\n","        cneu = dict_of_lists[\"cNEU\"][i]\n","        cagr = dict_of_lists[\"cAGR\"][i]\n","        ccon = dict_of_lists[\"cCON\"][i]\n","        copn = dict_of_lists[\"cOPN\"][i]\n","\n","        # Obtain fragments\n","        fragments = fragment_string(text, max_words)\n","\n","        # Add fragments\n","        for fragment in fragments:\n","            if len(fragment) >= min_fragm_len or len(fragments) == 1:\n","                add_dict_entry(fragm_dict_of_lists, authid, fragment, cext, cneu, cagr, ccon, copn)\n","\n","    # Create augmented dataframe\n","    augm_df = pd.DataFrame(fragm_dict_of_lists)\n","\n","    return augm_df"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"31OKBERNs29T"},"outputs":[],"source":["# Fragment data\n","max_words = 256\n","min_fragm_len = 256\n","df_fragm = fragment_essays_df(df, max_words, min_fragm_len)\n","fragmented_data_file = \"./df_essays_fragm_\"+str(max_words)+\".csv\"\n","df_fragm.to_csv(fragmented_data_file, index=False)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kHzzVSk5s29U"},"outputs":[],"source":["# Load fragmented data\n","fragmented_data_file = \"./df_essays_fragm_\"+str(max_words)+\".csv\"\n","df_fragm = pd.read_csv(fragmented_data_file, encoding = \"utf-8\")\n","print(df_fragm.head())\n","print(len(df), len(df_fragm))"]},{"cell_type":"markdown","metadata":{"id":"G2JiwLyTs29U"},"source":["### Data Split"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1AkAUjPms29U"},"outputs":[],"source":["random_state = 42\n","\n","# Split with validation and test sets\n","df_all_train, df_test = train_test_split(df, test_size=0.1, random_state=random_state)\n","df_train, df_valid = train_test_split(df_all_train, test_size=0.1, random_state=random_state)\n","print(\"Data with validation + test:\", len(df_train), len(df_valid), len(df_test))\n","\n","# Fragmented data split with validation and test sets\n","df_fragm_all_train, df_fragm_test = train_test_split(df_fragm, test_size=0.1, random_state=random_state, shuffle=True)\n","df_fragm_train, df_fragm_valid = train_test_split(df_fragm_all_train, test_size=0.1, random_state=random_state, shuffle=True)\n","print(\"Fragmented Data:\", len(df_fragm_train), len(df_fragm_valid), len(df_fragm_test))\n","df_fragm_train.to_csv(\"./df_essays_fragm_train.csv\", index=False)\n","df_fragm_valid.to_csv(\"./df_essays_fragm_valid.csv\", index=False)\n","df_fragm_test.to_csv(\"./df_essays_fragm_test.csv\", index=False)"]},{"cell_type":"markdown","metadata":{"id":"yYoh8V-lItvO"},"source":["## Extraversion"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JX9-Q9-3s29V"},"outputs":[],"source":["# FRAGMENTED DATA (256)\n","\n","# Define model name\n","model_name = \"bert-base-uncased\"\n","\n","# Define tokenizer\n","tokenizer = AutoTokenizer.from_pretrained(model_name, do_lower_case=True)\n","\n","# Create datasets\n","X_train, X_valid, y_train, y_valid = create_partition_from_split(df_fragm_train, df_fragm_valid, 'TEXT', 'cEXT')\n","_, X_test, _, y_test = create_partition_from_split(df_fragm_train, df_fragm_test, 'TEXT', 'cEXT')\n","\n","train_dataset, valid_dataset = create_datasets(tokenizer, X_train, X_valid, y_train, y_valid)\n","_, test_dataset = create_datasets(tokenizer, X_train, X_test, y_train, y_test)\n","\n","# Launch training experiment\n","suffix = \"extraversion_fragm\"\n","hidden_dropout_probs = [0.1, 0.2, 0.3]\n","attn_dropout_probs = [0.1, 0.2, 0.3]\n","lrs = [5e-5, 4e-5, 3e-5, 2e-5]\n","num_epochs = 4\n","training_experiment(suffix, hidden_dropout_probs, attn_dropout_probs, lrs, num_epochs,\n","                    model_name, train_dataset, valid_dataset, test_dataset)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PoOm8qWbs29W"},"outputs":[],"source":["# FRAGMENTED DATA (256) + ROBERTA\n","\n","# Define model name\n","model_name = \"FacebookAI/roberta-base\"\n","\n","# Define tokenizer\n","tokenizer = AutoTokenizer.from_pretrained(model_name, do_lower_case=True)\n","\n","# Create datasets\n","X_train, X_valid, y_train, y_valid = create_partition_from_split(df_fragm_train, df_fragm_valid, 'TEXT', 'cEXT')\n","_, X_test, _, y_test = create_partition_from_split(df_fragm_train, df_fragm_test, 'TEXT', 'cEXT')\n","\n","train_dataset, valid_dataset = create_datasets(tokenizer, X_train, X_valid, y_train, y_valid)\n","_, test_dataset = create_datasets(tokenizer, X_train, X_test, y_train, y_test)\n","\n","# Launch training experiment\n","suffix = \"extraversion_fragm_roberta\"\n","hidden_dropout_probs = [0.1, 0.2, 0.3]\n","attn_dropout_probs = [0.1, 0.2, 0.3]\n","lrs = [4e-5, 3e-5, 2e-5]\n","num_epochs = 4\n","training_experiment(suffix, hidden_dropout_probs, attn_dropout_probs, lrs, num_epochs,\n","                    model_name, train_dataset, valid_dataset, test_dataset,\n","                    mlp_classif=False, mlp_hsize_factor=1)"]},{"cell_type":"markdown","metadata":{"id":"J9YCoVVyIxJ-"},"source":["## Neuroticism"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PNLk1GLds29X"},"outputs":[],"source":["# FRAGMENTED DATA (256)\n","\n","# Define model name\n","model_name = \"bert-base-uncased\"\n","\n","# Define tokenizer\n","tokenizer = AutoTokenizer.from_pretrained(model_name, do_lower_case=True)\n","\n","# Create datasets\n","X_train, X_valid, y_train, y_valid = create_partition_from_split(df_fragm_train, df_fragm_valid, 'TEXT', 'cNEU')\n","_, X_test, _, y_test = create_partition_from_split(df_fragm_train, df_fragm_test, 'TEXT', 'cNEU')\n","\n","train_dataset, valid_dataset = create_datasets(tokenizer, X_train, X_valid, y_train, y_valid)\n","_, test_dataset = create_datasets(tokenizer, X_train, X_test, y_train, y_test)\n","\n","# Launch training experiment\n","suffix = \"neuroticism_fragm\"\n","hidden_dropout_probs = [0.1, 0.2, 0.3]\n","attn_dropout_probs = [0.1, 0.2, 0.3]\n","lrs = [5e-5, 4e-5, 3e-5, 2e-5]\n","num_epochs = 3\n","training_experiment(suffix, hidden_dropout_probs, attn_dropout_probs, lrs, num_epochs,\n","                    model_name, train_dataset, valid_dataset, test_dataset)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vmOS4lK-s29X"},"outputs":[],"source":["# FRAGMENTED DATA (256) + ROBERTA\n","\n","# Define model name\n","model_name = \"FacebookAI/roberta-base\"\n","\n","# Define tokenizer\n","tokenizer = AutoTokenizer.from_pretrained(model_name, do_lower_case=True)\n","\n","# Create datasets\n","X_train, X_valid, y_train, y_valid = create_partition_from_split(df_fragm_train, df_fragm_valid, 'TEXT', 'cNEU')\n","_, X_test, _, y_test = create_partition_from_split(df_fragm_train, df_fragm_test, 'TEXT', 'cNEU')\n","\n","train_dataset, valid_dataset = create_datasets(tokenizer, X_train, X_valid, y_train, y_valid)\n","_, test_dataset = create_datasets(tokenizer, X_train, X_test, y_train, y_test)\n","\n","# Launch training experiment\n","suffix = \"neuroticism_fragm_roberta\"\n","hidden_dropout_probs = [0.1, 0.2, 0.3]\n","attn_dropout_probs = [0.1, 0.2, 0.3]\n","lrs = [4e-5, 3e-5, 2e-5]\n","num_epochs = 4\n","training_experiment(suffix, hidden_dropout_probs, attn_dropout_probs, lrs, num_epochs,\n","                    model_name, train_dataset, valid_dataset, test_dataset,\n","                    mlp_classif=False, mlp_hsize_factor=1)"]},{"cell_type":"markdown","metadata":{"id":"_HrKemaZKsgY"},"source":["## Agreeableness"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"35qgvJ8Ns29Y"},"outputs":[],"source":["# FRAGMENTED DATA (256)\n","\n","# Define model name\n","model_name = \"bert-base-uncased\"\n","\n","# Define tokenizer\n","tokenizer = AutoTokenizer.from_pretrained(model_name, do_lower_case=True)\n","\n","# Create datasets\n","X_train, X_valid, y_train, y_valid = create_partition_from_split(df_fragm_train, df_fragm_valid, 'TEXT', 'cAGR')\n","_, X_test, _, y_test = create_partition_from_split(df_fragm_train, df_fragm_test, 'TEXT', 'cAGR')\n","\n","train_dataset, valid_dataset = create_datasets(tokenizer, X_train, X_valid, y_train, y_valid)\n","_, test_dataset = create_datasets(tokenizer, X_train, X_test, y_train, y_test)\n","\n","# Launch training experiment\n","suffix = \"agreeableness_fragm\"\n","hidden_dropout_probs = [0.1, 0.2, 0.3]\n","attn_dropout_probs = [0.1, 0.2, 0.3]\n","lrs = [3e-5, 2e-5]\n","num_epochs = 4\n","training_experiment(suffix, hidden_dropout_probs, attn_dropout_probs, lrs, num_epochs,\n","                    model_name, train_dataset, valid_dataset, test_dataset)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"djuSXUZds29Y"},"outputs":[],"source":["# FRAGMENTED DATA (256) + ROBERTA\n","\n","# Define model name\n","model_name = \"FacebookAI/roberta-base\"\n","\n","# Define tokenizer\n","tokenizer = AutoTokenizer.from_pretrained(model_name, do_lower_case=True)\n","\n","# Create datasets\n","X_train, X_valid, y_train, y_valid = create_partition_from_split(df_fragm_train, df_fragm_valid, 'TEXT', 'cAGR')\n","_, X_test, _, y_test = create_partition_from_split(df_fragm_train, df_fragm_test, 'TEXT', 'cAGR')\n","\n","train_dataset, valid_dataset = create_datasets(tokenizer, X_train, X_valid, y_train, y_valid)\n","_, test_dataset = create_datasets(tokenizer, X_train, X_test, y_train, y_test)\n","\n","# Launch training experiment\n","suffix = \"agreeableness_fragm_roberta\"\n","hidden_dropout_probs = [0.1, 0.2, 0.3]\n","attn_dropout_probs = [0.1, 0.2, 0.3]\n","lrs = [3e-5, 2e-5]\n","num_epochs = 3\n","training_experiment(suffix, hidden_dropout_probs, attn_dropout_probs, lrs, num_epochs,\n","                    model_name, train_dataset, valid_dataset, test_dataset,\n","                    mlp_classif=False, mlp_hsize_factor=1)"]},{"cell_type":"markdown","metadata":{"id":"nhy3QG3mUGsI"},"source":["## Conscientiousness"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dkv41ZFhs29Z"},"outputs":[],"source":["# FRAGMENTED DATA (256)\n","\n","# Define model name\n","model_name = \"bert-base-uncased\"\n","\n","# Define tokenizer\n","tokenizer = AutoTokenizer.from_pretrained(model_name, do_lower_case=True)\n","\n","# Create datasets\n","X_train, X_valid, y_train, y_valid = create_partition_from_split(df_fragm_train, df_fragm_valid, 'TEXT', 'cCON')\n","_, X_test, _, y_test = create_partition_from_split(df_fragm_train, df_fragm_test, 'TEXT', 'cCON')\n","\n","train_dataset, valid_dataset = create_datasets(tokenizer, X_train, X_valid, y_train, y_valid)\n","_, test_dataset = create_datasets(tokenizer, X_train, X_test, y_train, y_test)\n","\n","# Launch training experiment\n","suffix = \"conscientiousness_fragm\"\n","#hidden_dropout_probs = [0.1, 0.2, 0.3]\n","#attn_dropout_probs = [0.1, 0.2, 0.3]\n","#lrs = [5e-5, 4e-5, 3e-5, 2e-5]\n","#num_epochs = 3\n","hidden_dropout_probs = [0.1, 0.2, 0.3]\n","attn_dropout_probs = [0.1, 0.2, 0.3]\n","lrs = [3e-5, 2e-5]\n","num_epochs = 4\n","training_experiment(suffix, hidden_dropout_probs, attn_dropout_probs, lrs, num_epochs,\n","                    model_name, train_dataset, valid_dataset, test_dataset,\n","                    mlp_classif=False, mlp_hsize_factor=1)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5koyY0l0s29Z"},"outputs":[],"source":["# FRAGMENTED DATA (256) + ROBERTA\n","\n","# Define model name\n","model_name = \"FacebookAI/roberta-base\"\n","\n","# Define tokenizer\n","tokenizer = AutoTokenizer.from_pretrained(model_name, do_lower_case=True)\n","\n","# Create datasets\n","X_train, X_valid, y_train, y_valid = create_partition_from_split(df_fragm_train, df_fragm_valid, 'TEXT', 'cCON')\n","_, X_test, _, y_test = create_partition_from_split(df_fragm_train, df_fragm_test, 'TEXT', 'cCON')\n","\n","train_dataset, valid_dataset = create_datasets(tokenizer, X_train, X_valid, y_train, y_valid)\n","_, test_dataset = create_datasets(tokenizer, X_train, X_test, y_train, y_test)\n","\n","# Launch training experiment\n","suffix = \"conscientiousness_fragm_roberta\"\n","hidden_dropout_probs = [0.1, 0.2, 0.3]\n","attn_dropout_probs = [0.1, 0.2, 0.3]\n","lrs = [3e-5, 2e-5]\n","num_epochs = 4\n","training_experiment(suffix, hidden_dropout_probs, attn_dropout_probs, lrs, num_epochs,\n","                    model_name, train_dataset, valid_dataset, test_dataset,\n","                    mlp_classif=False, mlp_hsize_factor=1)"]},{"cell_type":"markdown","metadata":{"id":"yQMt4OajUinQ"},"source":["## Openness"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2dfp5Cmvs29g"},"outputs":[],"source":["# FRAGMENTED DATA (256)\n","\n","# Define model name\n","model_name = \"bert-base-uncased\"\n","\n","# Define tokenizer\n","tokenizer = AutoTokenizer.from_pretrained(model_name, do_lower_case=True)\n","\n","# Create datasets\n","X_train, X_valid, y_train, y_valid = create_partition_from_split(df_fragm_train, df_fragm_valid, 'TEXT', 'cOPN')\n","_, X_test, _, y_test = create_partition_from_split(df_fragm_train, df_fragm_test, 'TEXT', 'cOPN')\n","\n","train_dataset, valid_dataset = create_datasets(tokenizer, X_train, X_valid, y_train, y_valid)\n","_, test_dataset = create_datasets(tokenizer, X_train, X_test, y_train, y_test)\n","\n","# Launch training experiment\n","suffix = \"openness_fragm\"\n","hidden_dropout_probs = [0.1, 0.2, 0.3]\n","attn_dropout_probs = [0.1, 0.2, 0.3]\n","lrs = [5e-5, 4e-5, 3e-5, 2e-5]\n","num_epochs = 4\n","training_experiment(suffix, hidden_dropout_probs, attn_dropout_probs, lrs, num_epochs,\n","                    model_name, train_dataset, valid_dataset, test_dataset)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5rXXKRWks29h"},"outputs":[],"source":["# FRAGMENTED DATA (256) + ROBERTA\n","\n","# Define model name\n","model_name = \"FacebookAI/roberta-base\"\n","\n","# Define tokenizer\n","tokenizer = AutoTokenizer.from_pretrained(model_name, do_lower_case=True)\n","\n","# Create datasets\n","X_train, X_valid, y_train, y_valid = create_partition_from_split(df_fragm_train, df_fragm_valid, 'TEXT', 'cOPN')\n","_, X_test, _, y_test = create_partition_from_split(df_fragm_train, df_fragm_test, 'TEXT', 'cOPN')\n","\n","train_dataset, valid_dataset = create_datasets(tokenizer, X_train, X_valid, y_train, y_valid)\n","_, test_dataset = create_datasets(tokenizer, X_train, X_test, y_train, y_test)\n","\n","# Launch training experiment\n","suffix = \"openness_fragm_roberta\"\n","hidden_dropout_probs = [0.1, 0.2, 0.3]\n","attn_dropout_probs = [0.1, 0.2, 0.3]\n","lrs = [3e-5, 2e-5]\n","num_epochs = 4\n","training_experiment(suffix, hidden_dropout_probs, attn_dropout_probs, lrs, num_epochs,\n","                    model_name, train_dataset, valid_dataset, test_dataset,\n","                    mlp_classif=False, mlp_hsize_factor=1)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6kgtHE38s29h"},"outputs":[],"source":[]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[{"file_id":"1kGs_K59NM_REC0NcvYBRA4KqJfsLvR2H","timestamp":1697704309206}]},"kernelspec":{"display_name":"Python [conda env:pytorch]","language":"python","name":"conda-env-pytorch-py"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.7"}},"nbformat":4,"nbformat_minor":0}