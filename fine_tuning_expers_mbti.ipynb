{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"S9uyxVCpSB_m"},"outputs":[],"source":["# NOTE FOR LOCAL EXECUTION: use pytorch kernel and execute the code below to specify cuda device\n","import os\n","os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RM-HTfuLX4RQ"},"outputs":[],"source":["# install packages\n","!pip install torch transformers memory_profiler datasets accelerate nltk tweet-preprocessor\n","import time\n","import datetime\n","tic = time.time()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"iOa0uy-SbMtR"},"outputs":[],"source":["# import modules\n","import torch\n","import random\n","from transformers.file_utils import is_tf_available, is_torch_available, is_torch_tpu_available\n","from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments, AdamW\n","from datasets import load_metric\n","%load_ext memory_profiler\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import accuracy_score\n","import pandas as pd\n","import numpy as np"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ynEdSMsrSB_p"},"outputs":[],"source":["# Function to create partition from split\n","def create_partition_from_split(train, test, x_colname, y_colname):\n","    X_train = train[x_colname]\n","    X_test = test[x_colname]\n","    y_train = train[y_colname]\n","    y_test = test[y_colname]\n","    return X_train, X_test, y_train, y_test"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"IX7A4Y9TYFwa"},"outputs":[],"source":["# Function to define datasets\n","def create_datasets(tokenizer, X_train, X_test, y_train, y_test):\n","    # Convert input dataframes into lists\n","    X_train = X_train.tolist()\n","    X_test = X_test.tolist()\n","    y_train = y_train.tolist()\n","    y_test = y_test.tolist()\n","\n","    # Tokenize the text\n","    train_tokens = tokenizer(X_train, truncation=True, padding=True, max_length=512)\n","    valid_tokens = tokenizer(X_test, truncation=True, padding=True, max_length=512)\n","\n","    class MakeTorchData(torch.utils.data.Dataset):\n","        def __init__(self, tokens, labels):\n","            self.tokens = tokens\n","            self.labels = labels\n","\n","        def __getitem__(self, idx):\n","            item = {k: torch.tensor(v[idx]) for k, v in self.tokens.items()}\n","            item[\"labels\"] = torch.tensor([self.labels[idx]])\n","            return item\n","\n","        def __len__(self):\n","            return len(self.labels)\n","\n","    # Convert our tokenized data into a torch Dataset\n","    train_dataset = MakeTorchData(train_tokens, y_train)\n","    valid_dataset = MakeTorchData(valid_tokens, y_test)\n","\n","    return train_dataset, valid_dataset"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Ls6OMwxBYtgr"},"outputs":[],"source":["from sklearn.metrics import accuracy_score\n","auc_metric = load_metric(\"roc_auc\", trust_remote_code=True)\n","\n","def compute_metrics(eval_pred):\n","    # Calculate accuracy\n","    predictions, labels = eval_pred\n","    predictions = np.argmax(predictions, axis=1)\n","    accuracy = accuracy_score(y_true=labels, y_pred=predictions)\n","\n","    # Calculate AUC\n","    roc_auc = auc_metric.compute(prediction_scores=predictions, references=labels)[\"roc_auc\"]\n","\n","    return {\"accuracy\": accuracy, \"roc_auc\": roc_auc}"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"iHjUrRyUuReQ"},"outputs":[],"source":["from transformers import TrainerCallback\n","\n","class CustomCallback(TrainerCallback):\n","\n","    def __init__(self, trainer) -> None:\n","        super().__init__()\n","        self._trainer = trainer\n","\n","    def on_epoch_end(self, args, state, control, **kwargs):\n","        train_results = self._trainer.evaluate(eval_dataset=self._trainer.train_dataset, metric_key_prefix=\"train\")\n","        print(train_results)\n","        eval_results = self._trainer.evaluate(eval_dataset=self._trainer.eval_dataset, metric_key_prefix=\"eval\")\n","        print(eval_results)\n","        print()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"AFiVPzkwYKtc"},"outputs":[],"source":["# Create trainer\n","def create_trainer(suffix, model, train_dataset, valid_dataset, num_epochs=5,\n","                   load_best_model_at_end=True, optimizer=None):\n","    training_args = TrainingArguments(\n","      output_dir='./results_' + suffix,  # output directory\n","      num_train_epochs=num_epochs,       # total number of training epochs\n","      per_device_train_batch_size=16,    # batch size per device during training\n","      per_device_eval_batch_size=16,     # batch size for evaluation\n","      warmup_steps=500,                  # number of warmup steps for learning rate scheduler\n","      weight_decay=0.01,                 # strength of weight decay\n","      logging_dir='./logs_' + suffix,    # directory for storing logs\n","      load_best_model_at_end = load_best_model_at_end, # load the best model when finished training\n","      metric_for_best_model = \"eval_roc_auc\", # select the base metrics\n","      greater_is_better = True,\n","      logging_steps=100,                 # log & save weights each logging_steps\n","      #save_steps=100,\n","      #save_strategy=\"steps\",\n","      save_strategy=\"epoch\",\n","      save_total_limit=1,\n","      #evaluation_strategy=\"steps\",      # evaluate each `logging_steps`\n","      evaluation_strategy=\"epoch\",       # evaluate each epoch\n","    )\n","\n","    trainer = Trainer(\n","        model=model,                     # the instantiated Transformers model to be trained\n","        args=training_args,              # training arguments, defined above\n","        train_dataset=train_dataset,     # training dataset\n","        eval_dataset=valid_dataset,      # evaluation dataset\n","        compute_metrics=compute_metrics, # the callback that computes metrics of interest\n","        optimizers=(optimizer, None)\n","    )\n","    # NOTE: metric for best model not correctly applied when adding callback\n","    #trainer.add_callback(CustomCallback(trainer))\n","    return trainer"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YblzOGp4SB_q"},"outputs":[],"source":["import torch.nn as nn\n","from sklearn.model_selection import KFold\n","\n","def write_float_pair_to_file(file_path, float1, float2):\n","    with open(file_path, 'w') as file:\n","        file.write(f\"{float1} {float2}\\n\")\n","\n","def get_mlp_classifier(hidden_size, num_labels, hsize_factor=1):\n","    new_classifier = nn.Sequential(\n","        nn.Linear(hidden_size, int(hidden_size * hsize_factor)),\n","        nn.ReLU(),\n","        nn.Linear(int(hidden_size * hsize_factor), num_labels)\n","    )\n","    return new_classifier\n","\n","def instantiate_model(model_name, num_labels, hidden_dropout_prob, attention_probs_dropout_prob,\n","                      mlp_classif=False, mlp_hsize_factor=1, freeze_bert=False):\n","    # Define the model with the current dropout probabilities\n","    if \"distilbert-\" in model_name:\n","        model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2,\n","                                                                   dropout=hidden_dropout_prob,\n","                                                                   attention_dropout=attention_probs_dropout_prob)\n","    else:\n","        model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2,\n","                                                                   hidden_dropout_prob=hidden_dropout_prob,\n","                                                                   attention_probs_dropout_prob=attention_probs_dropout_prob)\n","\n","    if mlp_classif:\n","        model.classifier = get_mlp_classifier(model.config.hidden_size,\n","                                              model.config.num_labels,\n","                                              mlp_hsize_factor)\n","\n","    # Freeze BERT parameters if requested\n","    if freeze_bert:\n","        for param in model.bert.parameters():\n","            param.requires_grad = False\n","\n","    return model\n","\n","def single_train_exper(suffix, hidden_dropout_prob, attention_probs_dropout_prob, lr, num_epochs,\n","                       model_name, train_dataset, valid_dataset, test_dataset=None, load_best_model_at_end=True,\n","                       mlp_classif=False, mlp_hsize_factor=1, freeze_bert=False):\n","    model = instantiate_model(model_name,\n","                              num_labels=2,\n","                              hidden_dropout_prob=hidden_dropout_prob,\n","                              attention_probs_dropout_prob=attention_probs_dropout_prob,\n","                              mlp_classif=mlp_classif,\n","                              mlp_hsize_factor=mlp_hsize_factor,\n","                              freeze_bert=freeze_bert)\n","\n","    optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=1e-2)\n","\n","\n","    # Rest of your code for this specific model configuration\n","    # Train the model\n","    trainer = create_trainer(suffix, model, train_dataset, valid_dataset, num_epochs=num_epochs,\n","                             load_best_model_at_end=load_best_model_at_end, optimizer=optimizer)\n","    training_results = trainer.train()  # Capture the training results\n","\n","    # Evaluate the model using the validation data\n","    valid_results = trainer.evaluate()\n","\n","    # Evaluate the model using the test data if given\n","    if test_dataset is not None:\n","        test_results = trainer.evaluate(eval_dataset=test_dataset)\n","    else:\n","        test_results = None\n","\n","    return trainer, training_results, valid_results, test_results\n","\n","\n","def training_experiment(suffix, hidden_dropout_probs, attn_dropout_probs, learn_rates, num_epochs,\n","                        model_name, train_dataset, valid_dataset, test_dataset=None, mlp_classif=False,\n","                        mlp_hsize_factor = 1, freeze_bert=False):\n","    # Loop to iterate through all combinations\n","    best_eval_measure = None\n","    best_test_measure = None\n","    for hidden_dropout_prob in hidden_dropout_probs:\n","        for attention_probs_dropout_prob in attn_dropout_probs:\n","            for lr in lrs:\n","                # Print start message\n","                print(f\"* Model with hidden_dropout_prob={hidden_dropout_prob} , attention_probs_dropout_prob={attention_probs_dropout_prob} and lr={lr}:\")\n","\n","                trainer, training_results, valid_results, test_results = single_train_exper(suffix,\n","                                                                                   hidden_dropout_prob,\n","                                                                                   attention_probs_dropout_prob,\n","                                                                                   lr,\n","                                                                                   num_epochs,\n","                                                                                   model_name,\n","                                                                                   train_dataset,\n","                                                                                   valid_dataset,\n","                                                                                   test_dataset,\n","                                                                                   load_best_model_at_end=True,\n","                                                                                   mlp_classif=mlp_classif,\n","                                                                                   mlp_hsize_factor=mlp_hsize_factor,\n","                                                                                   freeze_bert=freeze_bert)\n","\n","                # Show results\n","                print(f\"Training Results: {training_results}\")\n","                print(f\"Validation Results: {valid_results}\")\n","                if test_dataset is not None:\n","                    print(f\"Test Results: {test_results}\")\n","                print(\"\")\n","\n","                # Save model if it is the best so far\n","                eval_measure = valid_results['eval_roc_auc']\n","                if best_eval_measure is None or best_eval_measure < eval_measure:\n","                    best_eval_measure = eval_measure\n","                    if test_dataset is not None:\n","                        best_test_measure = test_results['eval_roc_auc']\n","                    trainer.save_model()\n","\n","    # Write validation and test AUCs to file\n","    output_dir='./results_' + suffix\n","    write_float_pair_to_file(output_dir+\"/aucs.txt\", best_eval_measure, best_test_measure)\n","\n","    return best_eval_measure, best_test_measure\n","\n","from sklearn.model_selection import KFold\n","\n","def kfold_exper(df, x_colname, y_colname, n_splits, suffix, hidden_dropout_prob,\n","                attention_probs_dropout_prob, lr, num_epochs, model_name,\n","                mlp_classif=False, mlp_hsize_factor = 1, freeze_bert=False):\n","    sum_eval_measure = 0\n","\n","    # Initialize KFold object\n","    kf = KFold(n_splits=n_splits, random_state=42, shuffle=True)\n","\n","    # Iterate over folds\n","    for i, (train_indices, test_indices) in enumerate(kf.split(df)):\n","        print(f\"Fold {i}:\")\n","\n","        # Obtain dataframes\n","        df_train = df.iloc[train_indices]\n","        df_test = df.iloc[test_indices]\n","\n","        # Obtain datasets\n","        X_train, X_test, y_train, y_test = create_partition_from_split(df_train, df_test, x_colname, y_colname)\n","        train_dataset, valid_dataset = create_datasets(X_train, X_test, y_train, y_test)\n","\n","        # Execute train experiment\n","        _, training_results, valid_results, _ = single_train_exper(suffix,\n","                                                                   hidden_dropout_prob,\n","                                                                   attention_probs_dropout_prob,\n","                                                                   lr,\n","                                                                   num_epochs,\n","                                                                   model_name,\n","                                                                   train_dataset,\n","                                                                   valid_dataset,\n","                                                                   load_best_model_at_end=False,\n","                                                                   mlp_classif=mlp_classif,\n","                                                                   mlp_hsize_factor=mlp_hsize_factor,\n","                                                                   freeze_bert=freeze_bert)\n","\n","        # Show results\n","        print(f\"Training Results: {training_results}\")\n","        print(f\"Validation Results: {valid_results}\")\n","        print(\"\")\n","\n","        # Retrieve evaluation measure\n","        eval_measure = valid_results['eval_roc_auc']\n","        sum_eval_measure += eval_measure\n","        print(\"- Evaluation measure:\", eval_measure)\n","        print(\"\")\n","\n","    result = sum_eval_measure / n_splits\n","    print(\"Average of Evaluation Measures:\", result)\n","\n","    return result"]},{"cell_type":"markdown","metadata":{"id":"dQVdloazSB_r"},"source":["## Data Preparation"]},{"cell_type":"markdown","metadata":{"id":"Qu8QcNZZSB_s"},"source":["### Load Original Data"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"IpWxeGPwN0ri"},"outputs":[],"source":["# Set the directory and create the data\n","\n","#from google.colab import drive\n","#drive.mount('/content/drive')\n","\n","# Load the data into a pandas DataFrame | remember to have a similar structure in your Drive so that the data can be read properly.\n","#df = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/NLP & Personality/Fine tuning/MBTI/mbti_2.csv', encoding = \"latin-1\")\n","df = pd.read_csv('/home/dortiz/nlp/tasks/psico/mbti_2.csv',encoding = \"latin-1\")\n","\n","# Print a sample of the data to verify it's loaded correctly\n","print(df.head())"]},{"cell_type":"markdown","metadata":{"id":"wqYkCvL9SB_t"},"source":["### Preprocess Data"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CQvuNJ-sSB_t"},"outputs":[],"source":["import preprocessor as p\n","\n","df['text'] = df['text'].apply(p.clean)"]},{"cell_type":"markdown","metadata":{"id":"7eglNwQySB_t"},"source":["### Data Fragmentation"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Xun3ZYn4SB_t"},"outputs":[],"source":["def fragment_mbti_df(df, max_words, min_fragm_len):\n","\n","    def add_dict_entry(dict_of_lists, ty, text, ie, ns, ft, jp):\n","        dict_of_lists[\"type\"].append(ty)\n","        dict_of_lists[\"text\"].append(text)\n","        dict_of_lists[\"I/E\"].append(ie)\n","        dict_of_lists[\"N/S\"].append(ns)\n","        dict_of_lists[\"F/T\"].append(ft)\n","        dict_of_lists[\"J/P\"].append(jp)\n","\n","    def fragment_string(text, max_words):\n","        words = text.split()\n","        fragments = []\n","        current_fragment = []\n","\n","        for word in words:\n","            current_fragment.append(word)\n","            if len(current_fragment) == max_words:\n","                fragments.append(' '.join(current_fragment))\n","                current_fragment = []\n","\n","        if current_fragment:\n","            fragments.append(' '.join(current_fragment))\n","\n","        return fragments\n","\n","    # Convert DataFrame to a dictionary of lists\n","    dict_of_lists = df.to_dict(orient='list')\n","\n","    # Initialize fragmented dictionary\n","    fragm_dict_of_lists = {colname:[] for colname in dict_of_lists}\n","\n","    # Iterate over rows\n","    num_rows = len(dict_of_lists[\"text\"])\n","\n","    # Initialize dictionary to memoize best synonyms\n","    best_synonym_map = {}\n","\n","    for i in range(num_rows):\n","        # Retrieve column values\n","        ty = dict_of_lists[\"type\"][i]\n","        text = dict_of_lists[\"text\"][i]\n","        ie = dict_of_lists[\"I/E\"][i]\n","        ns = dict_of_lists[\"N/S\"][i]\n","        ft = dict_of_lists[\"F/T\"][i]\n","        jp = dict_of_lists[\"J/P\"][i]\n","\n","        # Obtain fragments\n","        fragments = fragment_string(text, max_words)\n","\n","        # Add fragments\n","        for fragment in fragments:\n","            if len(fragment) >= min_fragm_len or len(fragments) == 1:\n","                add_dict_entry(fragm_dict_of_lists, ty, fragment, ie, ns, ft, jp)\n","\n","    # Create augmented dataframe\n","    augm_df = pd.DataFrame(fragm_dict_of_lists)\n","\n","    return augm_df"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uZJ3c1gSSB_t"},"outputs":[],"source":["# Fragment data\n","max_words = 256\n","min_fragm_len = 256\n","df_fragm = fragment_mbti_df(df, max_words, min_fragm_len)\n","fragmented_data_file = \"./df_mbti_fragm_\"+str(max_words)+\".csv\"\n","df_fragm.to_csv(fragmented_data_file, index=False)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NMyb36aySB_t"},"outputs":[],"source":["# Load fragmented data\n","fragmented_data_file = \"./df_mbti_fragm_\"+str(max_words)+\".csv\"\n","df_fragm = pd.read_csv(fragmented_data_file, encoding = \"utf-8\")\n","print(df_fragm.head())\n","print(len(df), len(df_fragm))"]},{"cell_type":"markdown","metadata":{"id":"mGXz8mUHSB_u"},"source":["### Mask Words"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tMf9bRGVSB_u"},"outputs":[],"source":["import re\n","\n","mbti_words = {'ENFJ', 'ENFP', 'ENTJ', 'ENTP', 'ENFJs', 'ENFPs', 'ENTJs', 'ENTPs',\n","              'ESFJ', 'ESFP', 'ESTJ', 'ESTP', 'ESFJs', 'ESFPs', 'ESTJs', 'ESTPs',\n","              'INFJ', 'INFP', 'INTJ', 'INTP', 'INFJs', 'INFPs', 'INTJs', 'INTPs',\n","              'ISFJ', 'ISFP', 'ISTJ', 'ISTP', 'ISFJs', 'ISFPs', 'ISTJs', 'ISTPs',\n","              'INFX', 'INFx', 'INFJness', 'ISFX', 'ISFx', 'ESxx', 'Ixxx', 'ISTJish', 'EXFJ',\n","              'E', 'N', 'F', 'J', 'P',\n","              'Es', 'Ns', 'Ss', 'Fs', 'Ts', 'Js', 'Ps',\n","              'EN', 'FJ', 'FP', 'TJ', 'TP',\n","              'ENs', 'FJs', 'FPs', 'TJs', 'TPs',\n","              'NE', 'NI', 'SI', 'TE',\n","              'NEs', 'NIs', 'SIs', 'TEs',\n","              'TI', 'FI',\n","              'TIs', 'FIs',\n","              'NF', 'NT', 'SF', 'ST',\n","              \"SE\", \"FE\", \"PE\", \"JE\",\n","              \"SEs\", \"FEs\", \"PEs\", \"JEs\",\n","              'NFs', 'NTs', 'TJs', 'SFs', 'STs',\n","              'ENF', 'ENT', 'ESF', 'EST',\n","              'ENFs', 'ENTs', 'ESFs', 'ESTs',\n","              'NFJ', 'NFP', 'NTJ', 'NTP',\n","              'NFJs', 'NFPs', 'NTJs', 'NTPs',\n","              'SFJ', 'SFP', 'STJ', 'STP',\n","              'SFJs', 'SFPs', 'STJs', 'STPs',\n","              'INF', 'INT', 'ISF', 'IST',\n","              'INFs', 'INTs', 'ISFs', 'ISTs'}\n","\n","def mask_words_bert(text, words_to_replace):\n","    # Create a regular expression pattern to match the combinations\n","    pattern = r'\\b(?:' + '|'.join(re.escape(word) for word in words_to_replace) + r')\\b'\n","\n","    # Replace the matched combinations with a masking token (e.g., \"[MASK]\")\n","    masked_text, num_replacements = re.subn(pattern, '[UNK]', text, flags=re.IGNORECASE)\n","\n","    return masked_text, num_replacements\n","\n","def mask_words_roberta(text, words_to_replace):\n","    # Create a regular expression pattern to match the combinations\n","    pattern = r'\\b(?:' + '|'.join(re.escape(word) for word in words_to_replace) + r')\\b'\n","\n","    # Replace the matched combinations with a masking token (e.g., \"[MASK]\")\n","    masked_text, num_replacements = re.subn(pattern, '<unk>', text, flags=re.IGNORECASE)\n","\n","    return masked_text, num_replacements\n","\n","# Apply the masking function to the 'text' column and create a new column\n","df['masked_text_bert'], df['num_replacements'] = zip(*df['text'].apply(lambda x: mask_words_bert(x, mbti_words)))\n","df['masked_text_roberta'], _ = zip(*df['text'].apply(lambda x: mask_words_roberta(x, mbti_words)))\n","\n","# Calculate the sum of replacements\n","total_replacements = df['num_replacements'].sum()\n","print(\"total replacements: \", total_replacements)\n","\n","# Repeat for fragmented data\n","df_fragm['masked_text_bert'], df_fragm['num_replacements'] = zip(*df_fragm['text'].apply(lambda x: mask_words_bert(x, mbti_words)))\n","df_fragm['masked_text_roberta'], _ = zip(*df_fragm['text'].apply(lambda x: mask_words_roberta(x, mbti_words)))\n","\n","# Print the first five rows to check the result\n","print(df.head())"]},{"cell_type":"markdown","metadata":{"id":"5-kfRbDwSB_u"},"source":["### Mask Words Randomly"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"N2D6o2QXSB_u"},"outputs":[],"source":["import random\n","\n","def mask_words_randomly_bert(text, fraction):\n","    return ' '.join('[UNK]' if random.random() < fraction else word for word in text.split())\n","\n","def mask_words_randomly_roberta(text, fraction):\n","    return ' '.join('<unk>' if random.random() < fraction else word for word in text.split())\n","\n","def count_unk_bert(df):\n","    return df['random_masked_text_bert'].str.count('\\[UNK\\]').sum()\n","\n","def count_unk_roberta(df):\n","    return df['random_masked_text_roberta'].str.count('<unk>').sum()\n","\n","# Calculate total word count and fraction of masked words\n","total_word_count = sum(df['text'].apply(lambda x: len(x.split())))\n","masked_words_fraction = total_replacements/total_word_count\n","print(total_word_count, total_replacements, masked_words_fraction)\n","\n","# Set random seed\n","random.seed(42)\n","\n","# Apply the function to each row of the DataFrame using a lambda function and store results in a new column\n","df['random_masked_text_bert'] = df['text'].apply(lambda x: mask_words_randomly_bert(x, masked_words_fraction))\n","df_fragm['random_masked_text_bert'] = df_fragm['text'].apply(lambda x: mask_words_randomly_bert(x, masked_words_fraction))\n","df['random_masked_text_roberta'] = df['text'].apply(lambda x: mask_words_randomly_roberta(x, masked_words_fraction))\n","df_fragm['random_masked_text_roberta'] = df_fragm['text'].apply(lambda x: mask_words_randomly_roberta(x, masked_words_fraction))\n","\n","# Print the first five rows to check the result\n","print(df['random_masked_text_bert'].head())\n","\n","# Count the number of [UNK] occurrences in the new column\n","unk_count = count_unk_bert(df)\n","print(unk_count)"]},{"cell_type":"markdown","metadata":{"id":"eSV6RbpuSB_u"},"source":["### Data Split"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"B1W7KeK-SB_u"},"outputs":[],"source":["random_state = 42\n","\n","# Split with validation and test sets\n","df_all_train, df_test = train_test_split(df, test_size=0.1, random_state=random_state)\n","df_train, df_valid = train_test_split(df_all_train, test_size=0.1, random_state=random_state)\n","print(\"Data with validation + test:\", len(df_train), len(df_valid), len(df_test))\n","\n","# Fragmented data split with validation and test sets\n","df_fragm_all_train, df_fragm_test = train_test_split(df_fragm, test_size=0.1, random_state=random_state, shuffle=True)\n","df_fragm_train, df_fragm_valid = train_test_split(df_fragm_all_train, test_size=0.1, random_state=random_state, shuffle=True)\n","print(\"Fragmented Data:\", len(df_fragm_train), len(df_fragm_valid), len(df_fragm_test))\n","df_fragm_train.to_csv(\"./df_mbti_fragm_train.csv\", index=False)\n","df_fragm_valid.to_csv(\"./df_mbti_fragm_valid.csv\", index=False)\n","df_fragm_test.to_csv(\"./df_mbti_fragm_test.csv\", index=False)"]},{"cell_type":"markdown","metadata":{"id":"yYoh8V-lItvO"},"source":["## I/E"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kIdJJs7vSB_u"},"outputs":[],"source":["# VALID + TEST EXPERIMENT\n","\n","# Define model name\n","model_name = \"bert-base-uncased\"\n","\n","# Define tokenizer\n","tokenizer = AutoTokenizer.from_pretrained(model_name, do_lower_case=True)\n","\n","# Create datasets\n","X_train, X_valid, y_train, y_valid = create_partition_from_split(df_train, df_valid, 'text', 'I/E')\n","_, X_test, _, y_test = create_partition_from_split(df_train, df_test, 'text', 'I/E')\n","\n","train_dataset, valid_dataset = create_datasets(tokenizer, X_train, X_valid, y_train, y_valid)\n","_, test_dataset = create_datasets(tokenizer, X_train, X_test, y_train, y_test)\n","\n","# Launch training experiment\n","suffix = \"mbti_ie_valtest\"\n","hidden_dropout_probs = [0.1, 0.2]\n","attn_dropout_probs = [0.1, 0.2]\n","lrs = [4e-5, 3e-5, 2e-5]\n","num_epochs = 4\n","training_experiment(suffix, hidden_dropout_probs, attn_dropout_probs, lrs, num_epochs,\n","                    model_name, train_dataset, valid_dataset, test_dataset)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"g2-rptlLSB_v"},"outputs":[],"source":["# FRAGMENTED DATA (256)\n","\n","# Define model name\n","model_name = \"bert-base-uncased\"\n","\n","# Define tokenizer\n","tokenizer = AutoTokenizer.from_pretrained(model_name, do_lower_case=True)\n","\n","# Create datasets\n","X_train, X_valid, y_train, y_valid = create_partition_from_split(df_fragm_train, df_fragm_valid, 'text', 'I/E')\n","_, X_test, _, y_test = create_partition_from_split(df_fragm_train, df_fragm_test, 'text', 'I/E')\n","\n","train_dataset, valid_dataset = create_datasets(tokenizer, X_train, X_valid, y_train, y_valid)\n","_, test_dataset = create_datasets(tokenizer, X_train, X_test, y_train, y_test)\n","\n","# Launch training experiment\n","suffix = \"mbti_ie_fragm_256\"\n","hidden_dropout_probs = [0.1, 0.2]\n","attn_dropout_probs = [0.1, 0.2]\n","lrs = [4e-5, 3e-5, 2e-5]\n","num_epochs = 3\n","training_experiment(suffix, hidden_dropout_probs, attn_dropout_probs, lrs, num_epochs,\n","                    model_name, train_dataset, valid_dataset, test_dataset)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kGpo4PRnSB_v"},"outputs":[],"source":["# FRAGMENTED DATA (256) + ROBERTA\n","\n","# Define model name\n","model_name = \"FacebookAI/roberta-base\"\n","\n","# Define tokenizer\n","tokenizer = AutoTokenizer.from_pretrained(model_name, do_lower_case=True)\n","\n","# Create datasets\n","X_train, X_valid, y_train, y_valid = create_partition_from_split(df_fragm_train, df_fragm_valid, 'text', 'I/E')\n","_, X_test, _, y_test = create_partition_from_split(df_fragm_train, df_fragm_test, 'text', 'I/E')\n","\n","train_dataset, valid_dataset = create_datasets(tokenizer, X_train, X_valid, y_train, y_valid)\n","_, test_dataset = create_datasets(tokenizer, X_train, X_test, y_train, y_test)\n","\n","# Launch training experiment\n","suffix = \"mbti_ie_fragm_256_roberta\"\n","hidden_dropout_probs = [0.1, 0.2]\n","attn_dropout_probs = [0.1, 0.2]\n","lrs = [3e-5, 2e-5]\n","num_epochs = 3\n","training_experiment(suffix, hidden_dropout_probs, attn_dropout_probs, lrs, num_epochs,\n","                    model_name, train_dataset, valid_dataset, test_dataset)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qaWPuLMRSB_v"},"outputs":[],"source":["# MASKED + FRAGMENTED DATA (256) + ROBERTA\n","\n","# Define model name\n","model_name = \"FacebookAI/roberta-base\"\n","\n","# Define tokenizer\n","tokenizer = AutoTokenizer.from_pretrained(model_name, do_lower_case=True)\n","\n","# Create datasets\n","X_train, X_valid, y_train, y_valid = create_partition_from_split(df_fragm_train, df_fragm_valid,\n","                                                                 'masked_text_roberta',\n","                                                                 'I/E')\n","_, X_test, _, y_test = create_partition_from_split(df_fragm_train, df_fragm_test, 'masked_text_roberta', 'I/E')\n","\n","train_dataset, valid_dataset = create_datasets(tokenizer, X_train, X_valid, y_train, y_valid)\n","_, test_dataset = create_datasets(tokenizer, X_train, X_test, y_train, y_test)\n","\n","# Launch training experiment\n","suffix = \"masked_mbti_ie_fragm_256_roberta\"\n","hidden_dropout_probs = [0.1, 0.2]\n","attn_dropout_probs = [0.1, 0.2]\n","lrs = [3e-5, 2e-5]\n","num_epochs = 3\n","training_experiment(suffix, hidden_dropout_probs, attn_dropout_probs, lrs, num_epochs,\n","                    model_name, train_dataset, valid_dataset, test_dataset)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MUYb8WMmSB_v"},"outputs":[],"source":["# RANDOM MASKED + FRAGMENTED DATA (256)\n","\n","# Define model name\n","model_name = \"bert-base-uncased\"\n","\n","# Define tokenizer\n","tokenizer = AutoTokenizer.from_pretrained(model_name, do_lower_case=True)\n","\n","# Create datasets\n","X_train, X_valid, y_train, y_valid = create_partition_from_split(df_fragm_train, df_fragm_valid,\n","                                                                 'random_masked_text_bert',\n","                                                                 'I/E')\n","_, X_test, _, y_test = create_partition_from_split(df_fragm_train, df_fragm_test, 'random_masked_text_bert', 'I/E')\n","\n","train_dataset, valid_dataset = create_datasets(tokenizer, X_train, X_valid, y_train, y_valid)\n","_, test_dataset = create_datasets(tokenizer, X_train, X_test, y_train, y_test)\n","\n","# Launch training experiment\n","suffix = \"rand_masked_mbti_ie_fragm_256\"\n","hidden_dropout_probs = [0.1, 0.2]\n","attn_dropout_probs = [0.1, 0.2]\n","lrs = [3e-5, 2e-5]\n","num_epochs = 3\n","training_experiment(suffix, hidden_dropout_probs, attn_dropout_probs, lrs, num_epochs,\n","                    model_name, train_dataset, valid_dataset, test_dataset)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GRtEN4jnSB_v"},"outputs":[],"source":["# RANDOM MASKED + FRAGMENTED DATA (256) + ROBERTA\n","\n","# Define model name\n","model_name = \"FacebookAI/roberta-base\"\n","\n","# Define tokenizer\n","tokenizer = AutoTokenizer.from_pretrained(model_name, do_lower_case=True)\n","\n","# Create dadtasets\n","X_train, X_valid, y_train, y_valid = create_partition_from_split(df_fragm_train, df_fragm_valid,\n","                                                                 'random_masked_text_roberta',\n","                                                                 'I/E')\n","_, X_test, _, y_test = create_partition_from_split(df_fragm_train, df_fragm_test, 'random_masked_text_roberta', 'I/E')\n","\n","train_dataset, valid_dataset = create_datasets(tokenizer, X_train, X_valid, y_train, y_valid)\n","_, test_dataset = create_datasets(tokenizer, X_train, X_test, y_train, y_test)\n","\n","# Launch training experiment\n","suffix = \"rand_masked_mbti_ie_fragm_256_roberta\"\n","hidden_dropout_probs = [0.1, 0.2]\n","attn_dropout_probs = [0.1, 0.2]\n","lrs = [3e-5, 2e-5]\n","num_epochs = 3\n","training_experiment(suffix, hidden_dropout_probs, attn_dropout_probs, lrs, num_epochs,\n","                    model_name, train_dataset, valid_dataset, test_dataset)"]},{"cell_type":"markdown","metadata":{"id":"XVjaVdlGSB_w"},"source":["## N/S"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GdAYj8zGSB_w"},"outputs":[],"source":["# FRAGMENTED DATA (256)\n","\n","# Define model name\n","model_name = \"bert-base-uncased\"\n","\n","# Define tokenizer\n","tokenizer = AutoTokenizer.from_pretrained(model_name, do_lower_case=True)\n","\n","# Create datasets\n","X_train, X_valid, y_train, y_valid = create_partition_from_split(df_fragm_train, df_fragm_valid, 'text', 'N/S')\n","_, X_test, _, y_test = create_partition_from_split(df_fragm_train, df_fragm_test, 'text', 'N/S')\n","\n","train_dataset, valid_dataset = create_datasets(tokenizer, X_train, X_valid, y_train, y_valid)\n","_, test_dataset = create_datasets(tokenizer, X_train, X_test, y_train, y_test)\n","\n","# Launch training experiment\n","suffix = \"mbti_ns_fragm_256\"\n","hidden_dropout_probs = [0.1, 0.2]\n","attn_dropout_probs = [0.1, 0.2]\n","lrs = [4e-5, 3e-5, 2e-5]\n","num_epochs = 3\n","training_experiment(suffix, hidden_dropout_probs, attn_dropout_probs, lrs, num_epochs,\n","                    model_name, train_dataset, valid_dataset, test_dataset)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6PqNgO-5SB_w"},"outputs":[],"source":["# FRAGMENTED DATA (256) + ROBERTA\n","\n","# Define model name\n","model_name = \"FacebookAI/roberta-base\"\n","\n","# Define tokenizer\n","tokenizer = AutoTokenizer.from_pretrained(model_name, do_lower_case=True)\n","\n","# Create datasets\n","X_train, X_valid, y_train, y_valid = create_partition_from_split(df_fragm_train, df_fragm_valid, 'text', 'N/S')\n","_, X_test, _, y_test = create_partition_from_split(df_fragm_train, df_fragm_test, 'text', 'N/S')\n","\n","train_dataset, valid_dataset = create_datasets(tokenizer, X_train, X_valid, y_train, y_valid)\n","_, test_dataset = create_datasets(tokenizer, X_train, X_test, y_train, y_test)\n","\n","# Launch training experiment\n","suffix = \"mbti_ns_fragm_256_roberta\"\n","hidden_dropout_probs = [0.1, 0.2]\n","attn_dropout_probs = [0.1, 0.2]\n","lrs = [3e-5, 2e-5]\n","num_epochs = 3\n","training_experiment(suffix, hidden_dropout_probs, attn_dropout_probs, lrs, num_epochs,\n","                    model_name, train_dataset, valid_dataset, test_dataset)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3p3kHmKrSB_w"},"outputs":[],"source":["# MASKED + FRAGMENTED DATA (256)\n","\n","# Define model name\n","model_name = \"bert-base-uncased\"\n","\n","# Define tokenizer\n","tokenizer = AutoTokenizer.from_pretrained(model_name, do_lower_case=True)\n","\n","# Create datasets\n","X_train, X_valid, y_train, y_valid = create_partition_from_split(df_fragm_train, df_fragm_valid,\n","                                                                 'masked_text_bert',\n","                                                                 'N/S')\n","_, X_test, _, y_test = create_partition_from_split(df_fragm_train, df_fragm_test, 'masked_text_bert', 'N/S')\n","\n","train_dataset, valid_dataset = create_datasets(tokenizer, X_train, X_valid, y_train, y_valid)\n","_, test_dataset = create_datasets(tokenizer, X_train, X_test, y_train, y_test)\n","\n","# Launch training experiment\n","suffix = \"masked_mbti_ns_fragm_256\"\n","hidden_dropout_probs = [0.1, 0.2]\n","attn_dropout_probs = [0.1, 0.2]\n","lrs = [3e-5, 2e-5]\n","num_epochs = 3\n","training_experiment(suffix, hidden_dropout_probs, attn_dropout_probs, lrs, num_epochs,\n","                    model_name, train_dataset, valid_dataset, test_dataset)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JI3wYkrKSB_w"},"outputs":[],"source":["# MASKED + FRAGMENTED DATA (256) + ROBERTA\n","\n","# Define model name\n","model_name = \"FacebookAI/roberta-base\"\n","\n","# Define tokenizer\n","tokenizer = AutoTokenizer.from_pretrained(model_name, do_lower_case=True)\n","\n","# Create datasets\n","X_train, X_valid, y_train, y_valid = create_partition_from_split(df_fragm_train, df_fragm_valid,\n","                                                                 'masked_text_roberta',\n","                                                                 'N/S')\n","_, X_test, _, y_test = create_partition_from_split(df_fragm_train, df_fragm_test, 'masked_text_roberta', 'N/S')\n","\n","train_dataset, valid_dataset = create_datasets(tokenizer, X_train, X_valid, y_train, y_valid)\n","_, test_dataset = create_datasets(tokenizer, X_train, X_test, y_train, y_test)\n","\n","# Launch training experiment\n","suffix = \"masked_mbti_ns_fragm_256_roberta\"\n","hidden_dropout_probs = [0.1, 0.2]\n","attn_dropout_probs = [0.1, 0.2]\n","lrs = [3e-5, 2e-5]\n","num_epochs = 3\n","training_experiment(suffix, hidden_dropout_probs, attn_dropout_probs, lrs, num_epochs,\n","                    model_name, train_dataset, valid_dataset, test_dataset)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"84X4-WP0SB_z"},"outputs":[],"source":["# RANDOM MASKED + FRAGMENTED DATA (256)\n","\n","# Define model name\n","model_name = \"bert-base-uncased\"\n","\n","# Define tokenizer\n","tokenizer = AutoTokenizer.from_pretrained(model_name, do_lower_case=True)\n","\n","# Create datasets\n","X_train, X_valid, y_train, y_valid = create_partition_from_split(df_fragm_train, df_fragm_valid,\n","                                                                 'random_masked_text_bert',\n","                                                                 'N/S')\n","_, X_test, _, y_test = create_partition_from_split(df_fragm_train, df_fragm_test, 'random_masked_text_bert', 'N/S')\n","\n","train_dataset, valid_dataset = create_datasets(tokenizer, X_train, X_valid, y_train, y_valid)\n","_, test_dataset = create_datasets(tokenizer, X_train, X_test, y_train, y_test)\n","\n","# Launch training experiment\n","suffix = \"rand_masked_mbti_ie_fragm_256\"\n","hidden_dropout_probs = [0.1, 0.2]\n","attn_dropout_probs = [0.1, 0.2]\n","lrs = [3e-5, 2e-5]\n","num_epochs = 3\n","training_experiment(suffix, hidden_dropout_probs, attn_dropout_probs, lrs, num_epochs,\n","                    model_name, train_dataset, valid_dataset, test_dataset)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TlcnQZAySB_z"},"outputs":[],"source":["# RANDOM MASKED + FRAGMENTED DATA (256) + ROBERTA\n","\n","# Define model name\n","model_name = \"FacebookAI/roberta-base\"\n","\n","# Define tokenizer\n","tokenizer = AutoTokenizer.from_pretrained(model_name, do_lower_case=True)\n","\n","# Create dadtasets\n","X_train, X_valid, y_train, y_valid = create_partition_from_split(df_fragm_train, df_fragm_valid,\n","                                                                 'random_masked_text_roberta',\n","                                                                 'N/S')\n","_, X_test, _, y_test = create_partition_from_split(df_fragm_train, df_fragm_test, 'random_masked_text_roberta', 'N/S')\n","\n","train_dataset, valid_dataset = create_datasets(tokenizer, X_train, X_valid, y_train, y_valid)\n","_, test_dataset = create_datasets(tokenizer, X_train, X_test, y_train, y_test)\n","\n","# Launch training experiment\n","suffix = \"rand_masked_mbti_ns_fragm_256_roberta\"\n","hidden_dropout_probs = [0.1, 0.2]\n","attn_dropout_probs = [0.1, 0.2]\n","lrs = [3e-5, 2e-5]\n","num_epochs = 3\n","training_experiment(suffix, hidden_dropout_probs, attn_dropout_probs, lrs, num_epochs,\n","                    model_name, train_dataset, valid_dataset, test_dataset)"]},{"cell_type":"markdown","metadata":{"id":"21vUOc4fSB_0"},"source":["## F/T"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3pxHEvg1SB_0"},"outputs":[],"source":["# FRAGMENTED DATA (256)\n","\n","# Define model name\n","model_name = \"bert-base-uncased\"\n","\n","# Define tokenizer\n","tokenizer = AutoTokenizer.from_pretrained(model_name, do_lower_case=True)\n","\n","# Create datasets\n","X_train, X_valid, y_train, y_valid = create_partition_from_split(df_fragm_train, df_fragm_valid, 'text', 'F/T')\n","_, X_test, _, y_test = create_partition_from_split(df_fragm_train, df_fragm_test, 'text', 'F/T')\n","\n","train_dataset, valid_dataset = create_datasets(tokenizer, X_train, X_valid, y_train, y_valid)\n","_, test_dataset = create_datasets(tokenizer, X_train, X_test, y_train, y_test)\n","\n","# Launch training experiment\n","suffix = \"mbti_ft_fragm_256\"\n","hidden_dropout_probs = [0.1, 0.2]\n","attn_dropout_probs = [0.1, 0.2]\n","lrs = [3e-5, 2e-5]\n","num_epochs = 3\n","training_experiment(suffix, hidden_dropout_probs, attn_dropout_probs, lrs, num_epochs,\n","                    model_name, train_dataset, valid_dataset, test_dataset)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6RrkFtxLSB_0"},"outputs":[],"source":["# FRAGMENTED DATA (256) + ROBERTA\n","\n","# Define model name\n","model_name = \"FacebookAI/roberta-base\"\n","\n","# Define tokenizer\n","tokenizer = AutoTokenizer.from_pretrained(model_name, do_lower_case=True)\n","\n","# Create datasets\n","X_train, X_valid, y_train, y_valid = create_partition_from_split(df_fragm_train, df_fragm_valid, 'text', 'F/T')\n","_, X_test, _, y_test = create_partition_from_split(df_fragm_train, df_fragm_test, 'text', 'F/T')\n","\n","train_dataset, valid_dataset = create_datasets(tokenizer, X_train, X_valid, y_train, y_valid)\n","_, test_dataset = create_datasets(tokenizer, X_train, X_test, y_train, y_test)\n","\n","# Launch training experiment\n","suffix = \"mbti_ft_fragm_256_roberta\"\n","hidden_dropout_probs = [0.1, 0.2]\n","attn_dropout_probs = [0.1, 0.2]\n","lrs = [3e-5, 2e-5]\n","num_epochs = 3\n","training_experiment(suffix, hidden_dropout_probs, attn_dropout_probs, lrs, num_epochs,\n","                    model_name, train_dataset, valid_dataset, test_dataset)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZgXmj8oGSB_0"},"outputs":[],"source":["# MASKED + FRAGMENTED DATA (256)\n","\n","# Define model name\n","model_name = \"bert-base-uncased\"\n","\n","# Define tokenizer\n","tokenizer = AutoTokenizer.from_pretrained(model_name, do_lower_case=True)\n","\n","# Create datasets\n","X_train, X_valid, y_train, y_valid = create_partition_from_split(df_fragm_train, df_fragm_valid,\n","                                                                 'masked_text_bert',\n","                                                                 'F/T')\n","_, X_test, _, y_test = create_partition_from_split(df_fragm_train, df_fragm_test, 'masked_text_bert', 'F/T')\n","\n","train_dataset, valid_dataset = create_datasets(tokenizer, X_train, X_valid, y_train, y_valid)\n","_, test_dataset = create_datasets(tokenizer, X_train, X_test, y_train, y_test)\n","\n","# Launch training experiment\n","suffix = \"masked_mbti_ft_fragm_256\"\n","hidden_dropout_probs = [0.1, 0.2]\n","attn_dropout_probs = [0.1, 0.2]\n","lrs = [3e-5, 2e-5]\n","num_epochs = 3\n","training_experiment(suffix, hidden_dropout_probs, attn_dropout_probs, lrs, num_epochs,\n","                    model_name, train_dataset, valid_dataset, test_dataset)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"W9d9rFBWSB_0"},"outputs":[],"source":["# MASKED + FRAGMENTED DATA (256) + ROBERTA\n","\n","# Define model name\n","model_name = \"FacebookAI/roberta-base\"\n","\n","# Define tokenizer\n","tokenizer = AutoTokenizer.from_pretrained(model_name, do_lower_case=True)\n","\n","# Create datasets\n","X_train, X_valid, y_train, y_valid = create_partition_from_split(df_fragm_train, df_fragm_valid,\n","                                                                 'masked_text_roberta',\n","                                                                 'F/T')\n","_, X_test, _, y_test = create_partition_from_split(df_fragm_train, df_fragm_test, 'masked_text_roberta', 'F/T')\n","\n","train_dataset, valid_dataset = create_datasets(tokenizer, X_train, X_valid, y_train, y_valid)\n","_, test_dataset = create_datasets(tokenizer, X_train, X_test, y_train, y_test)\n","\n","# Launch training experiment\n","suffix = \"masked_mbti_ft_fragm_256_roberta\"\n","hidden_dropout_probs = [0.1, 0.2]\n","attn_dropout_probs = [0.1, 0.2]\n","lrs = [3e-5, 2e-5]\n","num_epochs = 3\n","training_experiment(suffix, hidden_dropout_probs, attn_dropout_probs, lrs, num_epochs,\n","                    model_name, train_dataset, valid_dataset, test_dataset)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RFLfJ8dZSB_0"},"outputs":[],"source":["# RANDOM MASKED + FRAGMENTED DATA (256)\n","\n","# Define model name\n","model_name = \"bert-base-uncased\"\n","\n","# Define tokenizer\n","tokenizer = AutoTokenizer.from_pretrained(model_name, do_lower_case=True)\n","\n","# Create datasets\n","X_train, X_valid, y_train, y_valid = create_partition_from_split(df_fragm_train, df_fragm_valid,\n","                                                                 'random_masked_text_bert',\n","                                                                 'F/T')\n","_, X_test, _, y_test = create_partition_from_split(df_fragm_train, df_fragm_test, 'random_masked_text_bert', 'F/T')\n","\n","train_dataset, valid_dataset = create_datasets(tokenizer, X_train, X_valid, y_train, y_valid)\n","_, test_dataset = create_datasets(tokenizer, X_train, X_test, y_train, y_test)\n","\n","# Launch training experiment\n","suffix = \"rand_masked_mbti_ft_fragm_256\"\n","hidden_dropout_probs = [0.1, 0.2]\n","attn_dropout_probs = [0.1, 0.2]\n","lrs = [3e-5, 2e-5]\n","num_epochs = 3\n","training_experiment(suffix, hidden_dropout_probs, attn_dropout_probs, lrs, num_epochs,\n","                    model_name, train_dataset, valid_dataset, test_dataset)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PcOOkDAjSB_0"},"outputs":[],"source":["# RANDOM MASKED + FRAGMENTED DATA (256) + ROBERTA\n","\n","# Define model name\n","model_name = \"FacebookAI/roberta-base\"\n","\n","# Define tokenizer\n","tokenizer = AutoTokenizer.from_pretrained(model_name, do_lower_case=True)\n","\n","# Create dadtasets\n","X_train, X_valid, y_train, y_valid = create_partition_from_split(df_fragm_train, df_fragm_valid,\n","                                                                 'random_masked_text_roberta',\n","                                                                 'F/T')\n","_, X_test, _, y_test = create_partition_from_split(df_fragm_train, df_fragm_test, 'random_masked_text_roberta', 'F/T')\n","\n","train_dataset, valid_dataset = create_datasets(tokenizer, X_train, X_valid, y_train, y_valid)\n","_, test_dataset = create_datasets(tokenizer, X_train, X_test, y_train, y_test)\n","\n","# Launch training experiment\n","suffix = \"rand_masked_mbti_ft_fragm_256_roberta\"\n","hidden_dropout_probs = [0.1, 0.2]\n","attn_dropout_probs = [0.1, 0.2]\n","lrs = [3e-5, 2e-5]\n","num_epochs = 3\n","training_experiment(suffix, hidden_dropout_probs, attn_dropout_probs, lrs, num_epochs,\n","                    model_name, train_dataset, valid_dataset, test_dataset)"]},{"cell_type":"markdown","metadata":{"id":"ZeaW6l_sSB_0"},"source":["## J/P"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qrA5uZRcSB_0"},"outputs":[],"source":["# FRAGMENTED DATA (256)\n","\n","# Define model name\n","model_name = \"bert-base-uncased\"\n","\n","# Define tokenizer\n","tokenizer = AutoTokenizer.from_pretrained(model_name, do_lower_case=True)\n","\n","# Create datasets\n","X_train, X_valid, y_train, y_valid = create_partition_from_split(df_fragm_train, df_fragm_valid, 'text', 'J/P')\n","_, X_test, _, y_test = create_partition_from_split(df_fragm_train, df_fragm_test, 'text', 'J/P')\n","\n","train_dataset, valid_dataset = create_datasets(tokenizer, X_train, X_valid, y_train, y_valid)\n","_, test_dataset = create_datasets(tokenizer, X_train, X_test, y_train, y_test)\n","\n","# Launch training experiment\n","suffix = \"mbti_jp_fragm_256\"\n","hidden_dropout_probs = [0.1, 0.2]\n","attn_dropout_probs = [0.1, 0.2]\n","lrs = [3e-5, 2e-5]\n","num_epochs = 3\n","training_experiment(suffix, hidden_dropout_probs, attn_dropout_probs, lrs, num_epochs,\n","                    model_name, train_dataset, valid_dataset, test_dataset)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Xt-PaeltSB_0"},"outputs":[],"source":["# FRAGMENTED DATA (256) + ROBERTA\n","\n","# Define model name\n","model_name = \"FacebookAI/roberta-base\"\n","\n","# Define tokenizer\n","tokenizer = AutoTokenizer.from_pretrained(model_name, do_lower_case=True)\n","\n","# Create datasets\n","X_train, X_valid, y_train, y_valid = create_partition_from_split(df_fragm_train, df_fragm_valid, 'text', 'J/P')\n","_, X_test, _, y_test = create_partition_from_split(df_fragm_train, df_fragm_test, 'text', 'J/P')\n","\n","train_dataset, valid_dataset = create_datasets(tokenizer, X_train, X_valid, y_train, y_valid)\n","_, test_dataset = create_datasets(tokenizer, X_train, X_test, y_train, y_test)\n","\n","# Launch training experiment\n","suffix = \"mbti_jp_fragm_256_roberta\"\n","hidden_dropout_probs = [0.1, 0.2]\n","attn_dropout_probs = [0.1, 0.2]\n","lrs = [3e-5, 2e-5]\n","num_epochs = 3\n","training_experiment(suffix, hidden_dropout_probs, attn_dropout_probs, lrs, num_epochs,\n","                    model_name, train_dataset, valid_dataset, test_dataset)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QJ7i-Y1OSB_0"},"outputs":[],"source":["# MASKED + FRAGMENTED DATA (256)\n","\n","# Define model name\n","model_name = \"bert-base-uncased\"\n","\n","# Define tokenizer\n","tokenizer = AutoTokenizer.from_pretrained(model_name, do_lower_case=True)\n","\n","# Create datasets\n","X_train, X_valid, y_train, y_valid = create_partition_from_split(df_fragm_train, df_fragm_valid,\n","                                                                 'masked_text_bert',\n","                                                                 'J/P')\n","_, X_test, _, y_test = create_partition_from_split(df_fragm_train, df_fragm_test, 'masked_text_bert', 'J/P')\n","\n","train_dataset, valid_dataset = create_datasets(tokenizer, X_train, X_valid, y_train, y_valid)\n","_, test_dataset = create_datasets(tokenizer, X_train, X_test, y_train, y_test)\n","\n","# Launch training experiment\n","suffix = \"masked_mbti_jp_fragm_256\"\n","hidden_dropout_probs = [0.1, 0.2]\n","attn_dropout_probs = [0.1, 0.2]\n","lrs = [3e-5, 2e-5]\n","training_experiment(suffix, hidden_dropout_probs, attn_dropout_probs, lrs, num_epochs,\n","                    model_name, train_dataset, valid_dataset, test_dataset)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WMBY9tNfSB_1"},"outputs":[],"source":["# MASKED + FRAGMENTED DATA (256) + ROBERTA\n","\n","# Define model name\n","model_name = \"FacebookAI/roberta-base\"\n","\n","# Define tokenizer\n","tokenizer = AutoTokenizer.from_pretrained(model_name, do_lower_case=True)\n","\n","# Create datasets\n","X_train, X_valid, y_train, y_valid = create_partition_from_split(df_fragm_train, df_fragm_valid,\n","                                                                 'masked_text_roberta',\n","                                                                 'J/P')\n","_, X_test, _, y_test = create_partition_from_split(df_fragm_train, df_fragm_test, 'masked_text_roberta', 'J/P')\n","\n","train_dataset, valid_dataset = create_datasets(tokenizer, X_train, X_valid, y_train, y_valid)\n","_, test_dataset = create_datasets(tokenizer, X_train, X_test, y_train, y_test)\n","\n","# Launch training experiment\n","suffix = \"masked_mbti_jp_fragm_256_roberta\"\n","hidden_dropout_probs = [0.1, 0.2]\n","attn_dropout_probs = [0.1, 0.2]\n","lrs = [3e-5, 2e-5]\n","num_epochs = 3\n","training_experiment(suffix, hidden_dropout_probs, attn_dropout_probs, lrs, num_epochs,\n","                    model_name, train_dataset, valid_dataset, test_dataset)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yDMhPsKrSB_1"},"outputs":[],"source":["# RANDOM MASKED + FRAGMENTED DATA (256)\n","\n","# Define model name\n","model_name = \"bert-base-uncased\"\n","\n","# Define tokenizer\n","tokenizer = AutoTokenizer.from_pretrained(model_name, do_lower_case=True)\n","\n","# Create datasets\n","X_train, X_valid, y_train, y_valid = create_partition_from_split(df_fragm_train, df_fragm_valid,\n","                                                                 'random_masked_text_bert',\n","                                                                 'J/P')\n","_, X_test, _, y_test = create_partition_from_split(df_fragm_train, df_fragm_test, 'random_masked_text_bert', 'J/P')\n","\n","train_dataset, valid_dataset = create_datasets(tokenizer, X_train, X_valid, y_train, y_valid)\n","_, test_dataset = create_datasets(tokenizer, X_train, X_test, y_train, y_test)\n","\n","# Launch training experiment\n","suffix = \"rand_masked_mbti_jp_fragm_256\"\n","hidden_dropout_probs = [0.1, 0.2]\n","attn_dropout_probs = [0.1, 0.2]\n","lrs = [3e-5, 2e-5]\n","num_epochs = 3\n","training_experiment(suffix, hidden_dropout_probs, attn_dropout_probs, lrs, num_epochs,\n","                    model_name, train_dataset, valid_dataset, test_dataset)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Xowo0cuZSB_1"},"outputs":[],"source":["# RANDOM MASKED + FRAGMENTED DATA (256) + ROBERTA\n","\n","# Define model name\n","model_name = \"FacebookAI/roberta-base\"\n","\n","# Define tokenizer\n","tokenizer = AutoTokenizer.from_pretrained(model_name, do_lower_case=True)\n","\n","# Create dadtasets\n","X_train, X_valid, y_train, y_valid = create_partition_from_split(df_fragm_train, df_fragm_valid,\n","                                                                 'random_masked_text_roberta',\n","                                                                 'J/P')\n","_, X_test, _, y_test = create_partition_from_split(df_fragm_train, df_fragm_test, 'random_masked_text_roberta', 'J/P')\n","\n","train_dataset, valid_dataset = create_datasets(tokenizer, X_train, X_valid, y_train, y_valid)\n","_, test_dataset = create_datasets(tokenizer, X_train, X_test, y_train, y_test)\n","\n","# Launch training experiment\n","suffix = \"rand_masked_mbti_jp_fragm_256_roberta\"\n","hidden_dropout_probs = [0.1, 0.2]\n","attn_dropout_probs = [0.1, 0.2]\n","lrs = [3e-5, 2e-5]\n","num_epochs = 3\n","training_experiment(suffix, hidden_dropout_probs, attn_dropout_probs, lrs, num_epochs,\n","                    model_name, train_dataset, valid_dataset, test_dataset)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wCrFZRQxSB_1"},"outputs":[],"source":[]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[{"file_id":"1kGs_K59NM_REC0NcvYBRA4KqJfsLvR2H","timestamp":1697704309206}]},"kernelspec":{"display_name":"Python [conda env:pytorch]","language":"python","name":"conda-env-pytorch-py"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.7"}},"nbformat":4,"nbformat_minor":0}